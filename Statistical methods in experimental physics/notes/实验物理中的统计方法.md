# 实验物理中的统计方法

## chap 01 基本概念

1. 随机事件：现象$A$有可能发生，也有可能不发生，那么现象$A$发生的事例叫做随机事件。

2. 概率

   柯尔莫戈夫公理：（概率的定义）概率是事件的函数。

   - $P(A)\geq0$
   - $P(S)=1$
   - 若$A\cap B=\varnothing$，则$P(A)+P(B)=P(A\cup B)$

3. 条件概率与独立性

   $P(A|B)=\frac{P(AB)}{P(B)}$

   $A$与$B$独立：$P(A|B)=P(A)$

   > $A\cap B=\varnothing$ 不能推出 AB独立无关。

4. 概率的诠释

   - 概率是相对频率（**可重复**实验中某个结果出现的次数占所有结果的比例）

   - **主观**概率（贝叶斯概率）

     **$P(A)=对A为真的信心程度$**

5. 贝叶斯定理与全概率公式
   $$
   P(A|B)=\frac{P(B|A)\cdot P(A)}{P(B)}
   $$

   $$
   P(B)=P(\cup_i(B\cap A_i))=\sum_i P(B\cap A_i)=\sum_iP(B|A_i)P(A_i)
   $$

   > 阐释：
   >
   > -  $P(A)$为理论的**先验概率**：“抛掷一枚硬币，正面反面向上的可能性各占一半”这一理论正确的概率
   > -  $P(B|A)$为理论预言的实验正确的概率：“硬币正面向上的概率为$\frac{1}{2}$”
   > -  $P(A|B)$为理论的**后验概率**
   >
   > 将$P(A|B)$作为下一次的先验概率，进行不断迭代。
   >
   > 例子：三门问题
   >
   > ![image-20250227174035630](https://raw.githubusercontent.com/stur007/img/main/img/202502271742784.png)
   >
   > ![image-20250227174051460](https://raw.githubusercontent.com/stur007/img/main/img/202502271740096.png)
   >
   > ![image-20250227174112483](https://raw.githubusercontent.com/stur007/img/main/img/202502271741787.png)

6. 随机变量与概率密度函数、累积分布函数

   概率密度函数 (p.d.f.)

   累积分布函数 (c.d.f.)

7. $\alpha$分位数、中位数、众数

   定义：
   $$
   F(x_\alpha)=\alpha
   $$
   按照数值小的一方面积分定义。

   即，$\alpha$-分位数称为下侧$\alpha$-分位数，当然也可以对应的定义上侧$\alpha$-分位数。

8. 直方图与概率密度函数

   联合概率密度分布函数$f(x,y)$

   边缘概率密度$f_x(x), f_y(y)$

   相互独立：
   $$
   f(x, y) = f_x(x)f_y(y)
   $$

9. 随机变量的函数

   - 一维随机变量

   函数 $a=a(x)$，$x$的概率密度函数$f(x)\mathrm{d}x$
   $$
   g(a)\mathrm{d}a=f(x)\mathrm{d}x
   $$
   得到：
   $$
   g(a)=f(x)\left|\frac{\mathrm{d}x(a)}{\mathrm{d}{a}}\right|
   $$

   - 多维随机变量的函数

     如：$z=xy$

     Mellin 卷积
     $$
     f(z)=\int_{-\infty}^\infty g(x)h(\frac{z}{x})\frac{\mathrm{d}x}{|x|}
     $$
     Flourier 卷积
     $$
     f(z)\int_{-\infty}^{\infty}g(x)h(z-x)\mathrm{d}x
     $$

   - Jacobi 行列式

     做变换，将我们不关心的函数部分积分掉，得到我们关心的概率。

10. 期待值与方差

    期待值、方差、标准差

    - 期待值

    $$
    \mu = E[x]=\int xf(x)\mathrm{d}x
    $$
    - 协方差

    $$
    \text{cov}[x, y] = E[(x-\mu_x)(y-\mu_y)]=E[xy]-\mu_x\mu_y
    $$
    - (Pearson)相关系数

    $$
    \rho_{x,y}=\frac{\text{cov}[x, y]}{\sigma_x\sigma_y}
    $$

    - 协方差矩阵$V$：

    $$
    V_{ij}=\text{cov}[x_i, x_j]
    $$

    由定义，协方差矩阵为对称矩阵。

    >  相关性定义的局限性：相关系数为0，但是不能说明x与y不相关。
    >
    >  其他的比较方式：秩相关性（只看顺序，不看数值）等等
    >
    >  ![](https://raw.githubusercontent.com/stur007/img/main/img/202503061653002.png)

    > 原点矩与中心矩
    > $$
    > \mu_k = E(X^k)
    > $$
    >
    > $$
    > \nu_k=E[(X-E(X))^k]
    > $$

    > 其他特征数：
    >
    > 偏度系数
    >
    > ![image-20250306170330021](https://raw.githubusercontent.com/stur007/img/main/img/202503061703410.png)

    

11. 特征函数

    对于随机变量$X$，称$\mathrm{e}^{\mathrm{i}tX}$的数学期望值为随机变量的特征函数。
    $$
    \varphi(t)=E[\mathrm{e}^{\mathrm{i}tX}]
    $$
    对于连续的情况，
    $$
    \varphi(t)=\int_{-\infty}^{\infty}\mathrm{e}^{\mathrm{i}tX}f(x)\mathrm{d}x
    $$
    特征函数的性质：

    如果$X$$Y$相互独立，则特征函数
    $$
    \varphi_{X+Y}(t)=E[\mathrm{e}^{\mathrm{i}t(X+Y)}]=E[\mathrm{e}^{\mathrm{i}tX}\mathrm{e}^{\mathrm{i}tY}]=\varphi_X(t)\varphi_Y(t)
    $$

    > 一例：
    >
    > 已知随机变量$X_1, ... , X_n$服从相同的概率分布$f(x)$且相互独立，则如何计算$X=\Sigma_{i=1}^nX_i$的概率分布函数？
    >
    > 解：
    >
    > 将卷积转化为乘积，都化成特征函数处理。（傅里叶变换的性质）

12. 不确定度的传递

    问题表述：

    假设我们对某个量测量了一组值 $\vec x= (x_1,…,x_n)$，并得到其 协方差 $𝑉_{ij}=\text{cov}[x_i,x_j]$（表征与$x_i$有关的测量不确定度）。 不确定度的传递(1) 73 现考虑一函数 $y=y(\vec x)$ ，如何求其方差$V[y]$？

    例：

    考虑随机变量$X_1, X_2$与不确定度$\sigma_1,\sigma_2$，计算$Y=f(X_1, X_2)$的不确定度？
    $$
    \sigma_Y=\sqrt{\left(\frac{\partial f}{\partial X_1}\right)^2\sigma_{X_1}^2+\left(\frac{\partial f}{\partial X_2}\right)^2\sigma_{X_2}^2+2\frac{\partial f}{\partial X_1}\frac{\partial f}{\partial X_2}\text{cov}[X_1, X_2]}
    $$
    **证明：**

    认为误差不太大，将 $Y$在平均值附近展开：
    $$
    y(\vec x)=y(\vec \mu)+\Sigma_{i=1}^n\frac{\partial y}{\partial x_i}(x_i-\mu_i)
    $$
    计算
    $$
    \sigma_y =E[y^2]-E[y]^2
    $$
    即可得到结果。

    >  随机变量之间的相关性往往与坐标选取有关，可以通过正交变换消除相关性。可以使用类似的方案得到结果：
    >
    > ![image-20250306175300884](https://raw.githubusercontent.com/stur007/img/main/img/202503061753236.png)
    >
    > 由于实对称矩阵可以正交相似对角化，所以一定可以找到对角化的协方差矩阵。

## chap 02 常用概率分布

### 二项分布

1. 定义

   $N$ 次**独立**测量(伯努利试验)，每次**只有**成功(概率始终为$p$) 或失败(概率为$1-p$)两种可能。

   定义随机变量$n$为成功的次数，则$n$服从二项分布：
   $$
   f(n;N,p)=\frac{N!}{n!(N-n)!}p^n(1-p)^{N-n}
   $$
   记作 $B(N, p)$。

2. 平均值与方差

   随机变量$n$的平均值
   $$
   E[n]=Np
   $$
   方差
   $$
   V[n]=Np(1-p)
   $$

   >例：计算探测效率与探测效率的不确定度。
   >
   >探测效率
   >$$
   >\varepsilon = \frac{N^\prime}{N}
   >$$
   >假设探测到的粒子数遵循二项分布，（$E(N^\prime)=N\varepsilon$）
   >
   >则
   >$$
   >\Delta \varepsilon = \frac{\Delta N}{N}=\frac{\sqrt {N\varepsilon (1-\varepsilon)}}{N}
   >$$

### 多项分布

- 测量的结果不止一种，将其中一种视为成功，其他视作失败，则变为二项分布。

### 泊松分布

- 泊松分布可以看作二项分布的极限近似，在$N\rightarrow \infty, p\rightarrow0,N\rightarrow\nu$的情况下，得到：

$$
f(n;\nu)=\frac{\nu^n}{n!}\mathrm e^{-\nu}
$$

- 均值
  $$
  E[n]=\nu
  $$
  方差
  $$
  V[n]=\nu
  $$
  泊松分布的特点：$n\pm \sqrt n$

- 泊松过程——泊松分布的产生

  条件：

  - 单位时间事件发生的概率固定
  - 同时发生两次事件几乎不可能（每次事件发生的概率低）
  - 每次事件发生是相互独立的

> 例：
>
> ![image-20250313161538529](https://raw.githubusercontent.com/stur007/img/main/img/202503131615965.png)
>
> N越大，p越小，泊松分布与二项分布越接近！
>
> 例：
>
> 假设某人站在路边想搭便车。 
>
> （1）每分钟过路的汽车数服从泊松分布，平均每分钟过路一辆，计算一小时通过的汽车数的分布。
>
> （2）每分钟过路的汽车数服从泊松分布，平均每分钟过路一辆。假设每辆车让搭便车的概率为 1%，并相互独立。 计算过了60辆车后还未能搭上车的概率。
>
> 解：
>
> 对于问题1，分布显然满足泊松分布的条件（时间可以无限细分，时间无限短的时候通过的汽车数目趋于零）。
>
> 对于问题2，分布满足二项分布的条件。
>
> 例：
>
> ![image-20250313163938120](https://raw.githubusercontent.com/stur007/img/main/img/202503131639561.png)

### 均匀分布

1. 函数形式
   $$
   f(x; \alpha, \beta)=\left \{
   \begin{aligned}
   \frac{1}{\beta -\alpha}, \alpha < x < \beta \\
   0, \mathrm{else}.
   \end{aligned}
   \right.
   $$

2. 均值与方差
   $$
   E[x]=\frac{\alpha+\beta}{2}
   $$

   $$
   V[x]=\frac{(\beta-\alpha)^2}{12}
   $$

   > 均匀分布是蒙特卡洛方法实现的基础！

### 指数分布

1. 函数形式
   $$
   f(x;\xi)=\left \{
   \begin{aligned}
   \frac{1}{\xi}\mathrm{e}^{-x/\xi}, x>0\\
   0, \mathrm{else}
   \end{aligned}
   \right.
   $$

2. 期望与方差
   $$
   E[x] =\xi, V[x]=\xi^2
   $$

   > 指数分布没有记忆性，可以任意选取开始位置！

### 高斯分布

1. 函数形式
   $$
   f(x;\mu,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}\mathrm e^{\frac{-(x-\mu)^2}{2\sigma^2}}
   $$
   ==记作 $N(\mu, \sigma^2)$== 。

2. 期望值
   $$
   E[x] =\mu
   $$
   方差
   $$
   V[x] =\sigma^2
   $$

   > 高斯分布是对连续变量概率分布的的很好近似。
   >
   > ![image-20250313165858705](https://raw.githubusercontent.com/stur007/img/main/img/202503131658172.png)
   >
   > 注意，变量$y$的分布形式与$x$原来的分布形式无关。

   > 多维高斯分布通过协方差矩阵定义：
   > $$
   > f(\vec x; \vec \mu, V)=\frac{1}{(2\pi)^{n/2}|V|^{1/2}}\mathrm e^{[\frac 1 2(\vec x-\vec \mu )^TV^{-1}(\vec x - \vec \mu) ]}
   > $$

   > 对于二维高斯分布可以证明

### 卡方分布

1. 定义

   对于相互独立的高斯随机变量$x_1, ..., x_n$，定义
   $$
   z= \sum_{i=1}^n \frac{(x_i-\mu_i)^2}{\sigma_i^2}
   $$
   得到$z$满足的分布：
   $$
   f(z;n)=\frac{1}{2^{n/2}\Gamma(n/2)}z^{n/2-1}\mathrm e^{-z/2}
   $$
   记作$\chi^2(n)$

2. 均值
   $$
   E[z] = n
   $$
   方差
   $$
   V[z]=2n
   $$

   > 例：最小二乘法的拟合优度检验
   > $$
   > y = f(x) = kx+b
   > $$
   > 改变$k$与$b$的取值，使得
   > $$
   > \chi^2 = \sum_{i=1}^n\frac{(y_i-f(x_i))^2}{\sigma_i^2}
   > $$
   > 极小。此时我们认为估计的结果是最好的。
   >
   > 不断重复实验，得到一系列卡方的值，这个值满足自由度为==$(n-2)$==的卡方分布。
   >
   > > 拟合优度：
   > >
   > > P值（假设检验）判断有多大的概率出错（理论模型与实际情况是否可以认为相符）

### 柯西分布

![image-20250313174659515](https://raw.githubusercontent.com/stur007/img/main/img/202503131747133.png)

### 朗道分布

![image-20250313174719858](https://raw.githubusercontent.com/stur007/img/main/img/202503131747324.png)

![image-20250313175419153](https://raw.githubusercontent.com/stur007/img/main/img/202503131754676.png)

### 贝塔分布

![image-20250313174739409](https://raw.githubusercontent.com/stur007/img/main/img/202503131747918.png)

> $\alpha = \beta = 1$变为均匀分布。

### 伽马分布

![image-20250313174759921](https://raw.githubusercontent.com/stur007/img/main/img/202503131748464.png)

### 学生氏分布

![image-20250313174812574](https://raw.githubusercontent.com/stur007/img/main/img/202503131748144.png)

## chap 03 蒙特卡罗方法 Monte Carlo Method

### 方法简介

#### 应用场景

1. 对于分布不方便解析求解；
2. 没有必要进行解析求解。
3. 处理高维数值定积分方面具有独特的优势。

#### 使用方法

- 使用MC方法计算定积分：
  1. 生成随机数序列$r_1, ..., r_n$；
  2. 按照函数$f(x)$生成随机数序列$x_1, ...x_n$；
  3. 根据函数所占比例计算积分值。
- 使用MC方法进行数据模拟。

### 随机数的产生

- 用物理方法产生真随机数：不可重复，产生速度慢
- 用数学方法产生伪随机数：可重复，产生速度快

随机数产生子

### 函数变换法与舍选法

#### 函数变换法

考虑随机变量$r= F(x)=\int f(x)\mathrm d x$
$$
g(r) \mathrm d r = f(x)\mathrm d x
$$

$$
g(r)=1
$$

故知随机变量$r$为均匀分布。

取$x = F^{-1}(r)$，随机生成0-1之间的均匀分布，就可以得到$x$的分布。

>对于指数分布$f(x)=\mathrm e^{-x}$，进行处理得到：
>$$
>r= F(x)=1-\mathrm e ^{-x}
>$$
>
>$$
>x = -\ln (1-r)
>$$
>
>实现代码如下：
>
>```python
>import ROOT
>import math
>rnd = ROOT.TRandom3()
>
>hist = ROOT.TH1F('exp_distribution', 'hist', 40, 0, 5)
>
>for i in range(10000):
>    hist.Fill(-math.log(1-rnd.Rndm()))
>
>canvas = ROOT.TCanvas('canvas', 'Canvas', 800, 600)
>hist.Draw()
>
>canvas.SaveAs('exp_distribution.png')
>```
>
>![image-20250320170549321](https://raw.githubusercontent.com/stur007/img/main/img/202503201705876.png)

### 蒙特卡罗方法在物理实验中的应用

### 习题

1. 公平的抽签：袋中有r个红球与b个黑球，现任意不放回地一一摸出，求事件第k次摸出红球的概率．

   解：
   $$
   P(K)=\frac r {r+b}
   $$
   样本空间
   $$
   n_\Omega = (r+b)!
   $$
   成功的次数
   $$
   n_A = r\cdot (r+b-1)!
   $$
   概率
   $$
   P(A) = \frac {n_A}{n_\Omega}= \frac r {r+b}
   $$

2. 生日问题：求任意n（n≤365）个人中，至少有两个人生日相同的概率。

   解：

   将事件转化成对立事件进行计算。$n$个人生日各不相同的概率为
   $$
   P(\overline A)= \frac {n_{\overline A}}{n_\Omega}=\frac{\frac {365!}{(365-n)!}}{365^n}=(1-\frac 1 {365})\cdots(1-\frac {n-1}{365})
   $$

   $$
   P(A)=1-P(\overline A)
   $$

   

3. 约会问题：小明和小红二人约定在[0,T]时段内去未名湖会面，规定先到者等候一段时间t(t≤T)再离开。试求小明和小红将会面的概率。

   解：

   两人出现的时间范围在$[0, T]$之间均匀分布，在两者事件相差的绝对值为$t$之内的情形可以相遇。
   $$
   P(A)=\frac{n_A}{n_\Omega}=1-\frac {(T-t)^2}{T^2}
   $$

4. 贝特朗奇论：1889 年，法国数学家贝特朗提出下述几何概率问题，并给出三种不同的答案。这就使有的人对当时的概率论中的一些概念与方法产生怀疑，因此被称作＂奇论＂。请思考： 在单位圆上任作一弦，求弦长大于$\sqrt 3$的概率．

   解：

   这与任意的方式具有相关性，对于任意的理解不同会导致结果不同。

   可以任意先选择圆周上的一个点，然后任意选择一个角度；或者任意选择弦的中点的位置。

   

5. 有n个人参加聚会，每个人都随身带了一份礼物，之后把礼物混合放在一起。会后每个人随机拿取一份礼物。求至少有一个人拿到了自己的礼物的概率。

   解：

   事件$A_i$代表第i个人拿到属于自己的礼物，则
   $$
   P(\cup _{i= 1}^nA_i)=\frac {C_n^1}{A_n^1}- \frac {C_n^2}{A_n^2}+\cdots+(-1)^{n+1}\frac{C_n^n}{A_n^n}=1-\frac 1{2!}+\frac 1{3!}-\cdots+(-1)^{n+1}\frac 1 {n!}
   $$

   

6. 有n个人参加聚会，每个人都随身带了一份礼物，之后把礼物混合放在一起。会后每个人随机拿取一份礼物。求恰好有 k 个人拿到了自己的礼物的概率。

   解：
   $$
   P= \frac 1{k!}\sum_{i= 1}^{n-k}\frac {(-1)^i}{i!}
   $$

7. 二项分布，泊松分布，高斯分布的特点以及转化关系

   

8.  

   ![image-20250327153610018](https://raw.githubusercontent.com/stur007/img/main/img/202503271536604.png)

   解：
   $$
   P(N_a=n_0)= \int_a^1 \mathrm d x_1\int_{a/x_1}^1\mathrm d x_2\cdots\int_{a/x_1\cdots x_{n_0-2}}^1\mathrm d x_{n_0-1}\int_0^{a/x_1\cdots x_{n_0-1}}\mathrm d x_{n_0}=\frac a{(n_0-1)!}(-\ln a)^{n_0-1}
   $$
   
9. $Z = XY, X+Y, Y/X, \text{min}{X,Y}, \text{max}{X,Y}$ 的随机变量分布

   实例：计算存在两种衰变方式时的粒子寿命。

   分析：

   设粒子的寿命为$x$，则粒子寿命小于等于$a$的概率（累积分布函数）：
   $$
   H(a)= P(x\leq a)= 1-P(x>a)=1-P(x_1>a)P(x_2>a)=1-[1-F(a)][1-G(a)]
   $$
   同理可以计算出
   $$
   P(x\leq a)=P(x\leq a_1)P(x\leq a_2)
   $$
   下面对实例进行处理：
   $$
   f(x) = \lambda_1\mathrm e ^{-\lambda_1 x}\\
   g(x)= \lambda_2\mathrm e^{-\lambda_2x}\\
   F(x)= 1-\mathrm e ^{-\lambda_1 x}\\
   G(x) = 1-\mathrm e^{-\lambda_2 x}\\
   H(x) = 1-\mathrm e^{-(\lambda_1 +\lambda_2) x}\\
   h(x)= (\lambda_1+\lambda_2)\mathrm e^{-(\lambda_1+\lambda_2)x}
   $$
   得到粒子寿命：
   $$
   \tau = \frac 1{\frac 1{\tau_1}+\frac 1{\tau_2}}
   $$
   

   例：

   $X, Y U(0, 1),Z=\min(X,\, Y)/\max (X,\, Y)$

   令$u = \min,\, v = \max$
   $$
   f(u, v)= 2, 0\leq u\leq v \leq 1
   $$
   
10. 某粒子在 A 气体中的平均自由程为 $L_A$，在 B 气体中的平均自由程为 $L_B$，求它在 AB 等量混合气体中的平均自由程

11. 习题 1.13， 1.14，1.15 讲解

    相关性：==线性相关==，即：$Y=X^2$的关系不能得到验证

    ![image-20250309104031036](https://raw.githubusercontent.com/stur007/img/main/img/202503091040100.png)

    构造统计量$U= Y-aX-b$，计算$\sigma [U]$, $E[U]$, 证明二者均为零即可。

    不妨取：
    $$
    b = E
    $$
    
12. 重期望法则
    $$
    E[E[X|Y]]=E[X]
    $$
    
13. Stein's lemma

14. 一名矿工被困在一个有三扇门的矿井中。如果选择 1 号门，3 小时后可以到达安全区域；如果选择 2 号门，5 小时后会回到原来的位置；如果选择 3 号门，7 小时会回到原来的位置。如果假设这名矿工对走过的门没有记忆，每次都以相等的概率选择每扇门，那么他平均需要多少时间才能到达安全区域？
    $$
    \begin{aligned}
    &E[X|Y=1]=3\\
    &E[X|Y=2]=5+E[X]\\
    &E[X|Y=3]=7+E[X]\\
    &E[X]= E[E[X|Y]]=\frac 1 3(E[X|Y=1]+E[X|Y=2]+E[X|Y=3])
    \end{aligned}
    $$
    可以解出$E[X]$。

15. Skewness（偏度） 与 Kurtosis（峰度）

## chap 04 统计检验

### 假设、检验、显著水平、功效、临界域

- 假设 hypothesis（pl. hypotheses）

  假设->物理理论：预测数据出现的概率，通过实验进行检验。

  简单假设：给出的假设$H_0$可以完全确定概率密度函数$f(x|H)$，称为假设的似然值（likelihood）$L(x|H)$.

  >  假设的似然值：假设成立时可观测量$x$出现的概率$f(x|H)$，即$x\sim f(x|H)$.

  符合假设：仍然包含未确定的参数。

- 检验

  零假设（null hypothesis）$H_0$与备择假设（alternative hypothesis）$H_1$。

  对简单假设$H_0$的检验：在假设$H_0$正确的情形下，在临界域$W$内观测到结果的==（小）==概率不超过$\alpha$，即
  $$
  P(x\in W|H_0)\leq \alpha
  $$
  $\alpha$：==显著性水平/检验大小==

  > 一般情形下$\alpha$的取值范围在${10^{-6}\sim 10^{-7}}$之间。

  ==临界域==：==拒绝域==，补集称为==接受域==。

- 第一类错误与第二类错误

  - 第一类错误：假设$H_0$为真，但是假设被拒绝。这类错误犯错的最大概率$P(x\in W|H_0)$就是显著性水平$\alpha$。
  - 第二类错误：假设$H_0$为假，但是假设$H_1$为真。这类错误的概率为$P(x\in S-W|H_1)=\beta$，$1-\beta$称为==功效==（检验对于排除备择假设$H_1$的效力）。

  > 一般而言，调整临界域不可能使得$\alpha$与$\beta$同时减小，但是<mark>增大样本容量</mark>可以实现这一点。

- 临界域的选择：单边检验与双边检验

### 信号本底的甄别

对于一次测量的实验结果通常不止是一个数值，而是一个向量$\vec x = (x_1, x_2, \cdots x_n)$，我们需要构造检验统计量来检验假设是否正确。

#### 检验统计量$t$

构造一个标量检验统计量$t=t(x_1, x_2, \cdots ,x_n)$，对假设进行检验。检验统计量也可以直接取为矢量$\vec t = \vec t(x_1, x_2, \cdots, x_n)$。

#### 纯度

事例选择的纯度：信号事例被正确分类的概率。（换言之，被判断为信号的所有事例中真正为信号的事例所占的比例。）
$$
P(s|x \in W)=\frac{P(x\in W|s)P(s)}{P(x \in W|s)P(s)+P(x \in W|b)P(b)}
$$

#### 检验效率

信号事例被接受的概率（被接受的信号事例占所有信号事例的比例）

即：
$$
\varepsilon_s = \int f(t|s)\mathrm d  t=1-\alpha
$$

### Neyman-Pearson引理与检验统计量的构造

对于多维统计检验量，应当如何选取==最佳==的临界域？

#### Neyman-Pearson引理

在给定效率的条件下，想要得到最高纯度的信号样本，接受域的选择可以为：
$$
\frac{g(\vec t|H_0)}{g(\vec t |H_1)}>c
$$
检验统计量可以取为
$$
r = \frac{g(\vec t|H_0)}{g(\vec t|H_1)}
$$

> 引理的证明(from https://chatgpt.com)
>
> 下面给出内曼–皮尔逊引理（Neyman–Pearson定理）的一个经典证明。假设我们要在两种简单假设之间做出检验：
>
> - 零假设 $H_0$：随机变量 $X$ 的概率密度（或概率质量）函数为 $p(x)$；
> - 备择假设 $H_1$：$X$ 的密度为 $q(x)$。
>
> 我们希望构造一个检验函数 $\phi(x)$，其中  
> $$
> \phi(x)=
> \begin{cases}
> 1, & \text{拒绝 } H_0,\\
> 0, & \text{接受 } H_0.
> \end{cases}
> $$
> （为简单起见，也可以允许在边界点上取介于 0 和 1 的值以调整精确水平。）
>
> 定理的结论是：在所有显著性水平不超过 $\alpha$ 的检验中，以似然比为依据的检验
> $$
> \phi^*(x)=
> \begin{cases}
> 1, & \text{当 } \displaystyle \frac{q(x)}{p(x)}>k,\\
> 0, & \text{当 } \displaystyle \frac{q(x)}{p(x)}<k,
> \end{cases}
> $$
> 
>
> （在 $\frac{q(x)}{p(x)}=k$ 的点上可能随机化，使得 $E_{H_0}[\phi^*(X)]=\alpha$）具有最大的统计功效，也就是说对于任何其它检验 $\phi(x)$（其第一类错误率不超过 $\alpha$），都有  
> $$
> E_{H_1}[\phi^*(X)]\ge E_{H_1}[\phi(X)].
> $$
> 下面是证明步骤：
>
> ---
>
> ### 1. 构造最优检验
>
> 设我们构造的似然比检验函数为
> $$
> \phi^*(x)=
> \begin{cases}
> 1, & \displaystyle \frac{q(x)}{p(x)}>k,\\
> \gamma(x), & \displaystyle \frac{q(x)}{p(x)}=k,\\
> 0, & \displaystyle \frac{q(x)}{p(x)}<k,
> \end{cases}
> $$
> 其中 $k>0$ 和 $\gamma(x)\in[0,1]$ 取值，使得该检验的显著性水平满足
> $$
> E_{H_0}[\phi^*(X)] = \int \phi^*(x)p(x)\,dx = \alpha.
> $$
>
> ---
>
> ### 2. 对任一水平不超过 $\alpha$ 的检验比较功效
>
> 设任一其它检验函数为 $\phi(x)$，满足
> $$
> E_{H_0}[\phi(X)] = \int \phi(x)p(x)\,dx \le \alpha.
> $$
> 我们要证明
> $$
> E_{H_1}[\phi^*(X)] - E_{H_1}[\phi(X)] \ge 0.
> $$
>
> 为此，考虑积分
> $$
> I=\int \bigl[\phi^*(x)-\phi(x)\bigr]\Bigl[q(x)-k\,p(x)\Bigr]\,dx.
> $$
>
> **注意：** 根据 $\phi^*(x)$ 的构造，
> - 当 $\frac{q(x)}{p(x)}>k$ 时，$\phi^*(x)=1$；此时 $q(x)-k\,p(x)>0$，故 $\phi^*(x)-\phi(x)\ge 0$ 时乘上正数得非负；
> - 当 $\frac{q(x)}{p(x)}<k$ 时，$\phi^*(x)=0$；此时 $q(x)-k\,p(x)<0$，而 $\phi^*(x)-\phi(x)=-\phi(x)\le 0$，乘上负数依然非负；
> - 在 $\frac{q(x)}{p(x)}=k$ 的点上，由于可以通过随机化调整，积分对这一处的贡献为零（或可以忽略）。
>
> 因此可以证明 $I\ge 0$。
>
> ---
>
> ### 3. 将积分 $I$ 与各检验的期望联系起来
>
> 将 $I$ 展开得：
> $$
> I=\int \phi^*(x)q(x)\,dx - k\int \phi^*(x)p(x)\,dx - \left[\int \phi(x)q(x)\,dx - k\int \phi(x)p(x)\,dx\right].
> $$
> 也就是说，
> $$
> E_{H_1}[\phi^*(X)] - k\,E_{H_0}[\phi^*(X)] - \left[E_{H_1}[\phi(X)] - k\,E_{H_0}[\phi(X)]\right] \ge 0.
> $$
>
> 由于按构造 $E_{H_0}[\phi^*(X)]=\alpha$ 而 $E_{H_0}[\phi(X)]\le \alpha$，则
> $$
> E_{H_1}[\phi^*(X)] - k\alpha \ge E_{H_1}[\phi(X)] - k\,E_{H_0}[\phi(X)] \ge E_{H_1}[\phi(X)] - k\alpha.
> $$
> 从而可得
> $$
> E_{H_1}[\phi^*(X)]\ge E_{H_1}[\phi(X)].
> $$
>
> 这说明在所有显著性水平不超过 $\alpha$ 的检验中，似然比检验 $\phi^*$ 的功效是最大的。
>
> ---
>
> ### 4. 总结
>
> 以上证明利用了如下关键思想：  
> - 将两种检验函数之差乘以 $q(x)-k\,p(x)$ 后积分，利用似然比检验的构造保证该乘积非负。  
> - 从而证明了似然比检验在 $H_1$ 下的期望（功效）至少不低于任何其它同水平检验。
>
> 这便是内曼–皮尔逊引理的证明，其结论说明在两个简单假设的情形下，基于似然比的检验是均匀最优的，即在控制第一类错误率（显著性水平）的条件下，其功效最大。
>
> ---
>
> ### 参考
>
> 本证明的思路与标准教材中给出的证明类似，例如参见  
> - [维基百科：内曼–皮尔逊引理](https://zh.wikipedia.org/wiki/%E5%86%85%E6%9B%BC%E2%80%93%E7%9A%AE%E5%B0%94%E6%A3%AE%E5%BC%95%E7%90%86)  
> - [知乎：Neyman–Pearson基本引理证明](https://zhuanlan.zhihu.com/p/348803654)
>
> 希望这个证明能帮助你理解内曼–皮尔逊定理的基本原理和数学推导过程。

#### 线性检验统计量、费舍尔(Fisher)甄别函数

$$
J(\boldsymbol a)= \frac{\boldsymbol a^TB\boldsymbol a}{\boldsymbol a^TW\boldsymbol a}
$$

注意到这是关于$a$的齐次函数，所以需要在固定$a$的模长的情况下进行偏导数运算。

不妨假设 $\boldsymbol aW\boldsymbol a^T=1$，得到
$$
J(\boldsymbol a)=\boldsymbol a^TB\boldsymbol a-\lambda(\boldsymbol a^TW\boldsymbol a-1)
$$
对其求偏导数$\frac{\partial J(\boldsymbol a)}{\partial \boldsymbol a}$得到
$$
B\boldsymbol a=\lambda W\boldsymbol a
$$

$$
W^{-1}B\boldsymbol a=\lambda \boldsymbol a
$$

即$\boldsymbol a$是矩阵$W^{-1}B$的特征向量。

再注意到
$$
\begin{aligned}
B_{ij}a_j=(\mu_0-\mu_1)_i&\boxed{(\mu_0-\mu_1)_ja_j}=\lambda_a(\mu_0-\mu_1)_i\\
&= \lambda_a
\end{aligned}
$$
最终得到：
$$
a \propto W^{-1}(\mu_0-\mu_1)
$$

在高斯分布同（协）方差分布的情形下，Fisher检验统计量给出的结果与Neymann-Pearson引理的结果相同。在其他情形下并不相同，但是可以通过做变换实现更好的结果。

![image-20250619103629710](https://raw.githubusercontent.com/stur007/img/main/img/202506191239448.png)

> 神经网络：
>
> - 单层感知器：
>
>   输入层->输出层
>
> ![image-20250417173957171](https://raw.githubusercontent.com/stur007/img/main/img/202504171740088.png)
>
> - 多层感知器：
>
>   输入层->隐藏层->输出层
>
> ![image-20250417174025444](https://raw.githubusercontent.com/stur007/img/main/img/202504171740986.png)
>
> 确定中间的参数是机器学习核心的内容！
>
> - 过度训练(overfitting)
>
>   参数过多，对训练数据的符合非常好，但是反而不能准确预测其他数据的结果。
>
> ![image-20250417174520708](https://raw.githubusercontent.com/stur007/img/main/img/202504171745660.png)
>
> - 输入变量选择
>
>   如果输入变量的相关性较强，常常对结果的影响不大。如，$x^2, x^4$的结果常常区别不是很大，一般情形下不需要保留高阶矩。

### 检验拟合优度、p值的定义与应用

#### p值的定义

对于给定的简单假设$H_0$，我们可以可以预测出各个结果出现的概率。实验中的结果是其中一种，考虑预期结果不好于实验结果的概率，定义为$p$值。即：
$$
p=观测到数据\vec x 与假设𝐻的符合程度不好于 实际数据\vec x_{obs}与𝐻的符合程度的概率
$$

> 例子：在抛硬币实验中，一共抛掷20次硬币，实验结果为17次正面向上。但是，实验预期的最有可能出现的结果为10。所以，与实验结果相比，更糟糕或者一样糟糕的预期结果为0，1，2，3，17，18，19，20。根据二项式分布的计算可以得到：
> $$
> p= P(N= 0,1,2,3,17,18,19,20)=0.0026
> $$
> 实验中，想要检验假设是否正确，可以重复实验，检测这些极端事例出现的概率是否与$p$值相符。（==假设$H_0$为真的概率并不是$p$值==）

对于这一实验结果，能不能认为假设$H_0$并不成立？换言之，有多少信心（Bayes主观概率的角度）认为原假设$H_0$正确或者错误？这是假设检验中的重要问题。

#### $H_0$的检验

由$p$值的定义，容易知道：
$$
P(p_0\leq \alpha)=\alpha
$$

> 证明：
> $$
> p(t) = \int _t^\infty f(x)\mathrm d x\\
> g(p)= 1(累积分布的随机变量是均匀分布)\\
> \alpha = \int _{t_0}^\infty f(x)\mathrm d x\\
> P(p_0\leq \alpha)=\int_0^\alpha g(p)\mathrm d p=\alpha
> $$

### 信号观测的显著程度

考虑泊松计数实验（假设信号与本底的测量结果均遵循泊松分布），设本底数的均值为$b$，信号的均值为$s$，$H_0$为没有信号产生，则：
$$
P(n|H_0)=\frac{b^n}{n!}\mathrm e ^{-b}
$$
假设$b=0.5,\,n=5$，问是否可以认为发现了信号？

解：

计算p值：
$$
p = \sum_{n=5}^\infty\frac{b^n}{n!}\mathrm e ^{-b}=1.7\times10^{-4}
$$
>使用分部积分法可以证明，泊松分布的部分和与卡方分布的累积分布函数之间有密切的联系，满足：
>$$
>\sum_{n=0}^m\frac{\nu^n}{n!}\mathrm e ^{-\nu}=1-F_{\chi^2}(2\nu;n_d=2(m+1))
>$$
>
>$$
>F_{\chi^2}(2\nu;n_d=2(m+1))=\int_{-\infty}^{2\nu}f_{\chi^2}(n_d=2(m+1))\mathrm d x
>$$

显著性$Z$：高斯变量在一个方向涨落得到相同𝑝值所对应的标准差的倍数。

与$p$值之间的一一对应关系：
$$
Z= \Phi^{-1}(1-p)
$$
$\Phi$为标准高斯分布$N(0, 1^2)$。当$\Phi>5(p<3\times10^{-7})$时认为原假设可能不成立。

### Pearson $\chi^2$检验

#### $\chi^2$统计量的定义

实验观测值：$\vec n =(n_1, n_2, ..., n_N)$

理论预期值：$\vec \mu = (\mu_1, \mu_2, ...\mu_N)$
$$
\chi^2=\sum_{i=1}^N\frac{(n_i-\mu_i)^2}{\sigma_i^2}
$$

对于变量为高斯分布的情形，$\chi^2$服从$\chi^2$分布。

#### $\chi^2$分布的性质

对于自由度数目为$n$的卡方分布，均值与期望：
$$
E[\chi^2]=n,V[\chi^2]=2n
$$
对于不服从卡方分布的值，使用MC方法。

## chap 05 参数估计的一般概念

### 估计量
#### 问题
对于测量结果$\vec x_i$，对于概率密度函数的参数有估计$\hat \theta_i$，问，应该如何对数据进行处理，得到最佳的估计结果？即，如何通过$\hat \theta$估计$\theta$?

如果用更加数学的语言进行表述，在实际操作过程中，我们并不知道概率密度函数$f(\vec x;\theta)$的具体形式，只能通过$n$次测量的结果$\vec x_1, ..., \vec x_n$对概率密度函数的形式进行估计。

> 测量结果$\vec x$称为**样本**，估计量是样本的函数。经过多次测量，可以推断出估计量的分布$g(\hat \theta, \theta)$。

#### 对估计量的要求
- 相合性：在样本容量无限大的极限下，估计量趋近于真值。
- 无偏性：$b= E[\hat \theta]-\theta$越小越好。
- 有效性：方差$V[\hat \theta]$越小越好。

定义均方误差MSE
$$
MSE=E[(\hat \theta-\theta)^2]=V[\hat \theta]+b^2
$$

> 对于偏倚为0，方差最小的估计量称为有效估计量。在$n\to \infty$的情形下偏倚趋近于0的估计量称为渐进有效估计量。

然而，在实际操作时，减小估计量的方差与减小估计量的偏移是相互矛盾的，要进行权衡。
### 均值的估计
#### 问题
对于随机变量$X$，具有未知的概率密度函数$f(x)$，想要通过有限次<mark>独立</mark>测量得到 $E[X]= \int f(x) \mathrm d x$ 的估计量 $\hat E[X]$，问应该如何构造函数得到的这一估计值？
> 显然，这一估计量为无参数估计量。
#### 函数构造
构造函数
$$
\hat \mu = \frac{1}{n}\sum_{i=1}^nx_i=\overline x
$$
#### 函数性质检验
##### 相合性
$$
\lim_{n\to\infty}\hat \mu = \mu
$$
(大数弱定律保证)
##### 偏倚
$$
E[\hat \mu] = E[\overline x] = \mu
$$
##### 估计量的方差
$$
\begin{aligned}
V[\hat \mu] &= E[\hat \mu^2]-E[\hat \mu]^2\\
&=\frac{\sigma^2}{n}
\end{aligned}
$$

### 方差的估计
#### 函数构造
假设**分布的均值与方差**都不能提前确定精确的值，构造函数：
$$
s^2 = \frac{1}{n-1}(\sum_{i=1}^{n}x_i^2-\overline x^2)
$$

> 注意，这里的公式与样本的方差公式不同，分母减去1的作用是，保证估计量是无偏估计量。

#### 函数性质检验
##### 方差估计量的偏倚
$$
E[s^2] = \mu_2
$$
##### 方差估计量的方差

![20250508165803](https://raw.githubusercontent.com/stur007/img/main/img/20250508165803.png)

### 样本协方差与相关系数
![20250508165917](https://raw.githubusercontent.com/stur007/img/main/img/20250508165917.png)

## chap 06 极大似然法
### 似然函数、极大似然估计量
#### 最大似然法的基本思想

假设函数分布的形式$f(x; \theta)$，对于多次独立的测量结果$\vec x = (x_1, ... , x_n)$，如何估计$\vec \theta = (\theta_1, ..., \theta_m)$?

$$
P(x_i \in [x_i, x_i + \mathrm d x_i])= \prod_{i=1}^nf(x_i; \theta)\mathrm d x_i
$$

直观上，我们有理由相信，<mark>观测到的结果具有相对较高的概率值</mark>。

定义似然函数：
$$
L(\vec x, \theta)= \prod_{i=1}^n f(x_i; \theta)
$$

对数似然函数：
$$
\ln L(\vec x, \theta)= \sum_{i=1}^n\ln f(x_i; \theta)
$$

> 对于给定的一组实验数据，似然函数就是估计量的函数。

最大似然估计量（Maximum Likelihood Estimator）定义为：使得似然函数取最大值的估计量，满足：
$$
\frac{\partial \ln L(\vec \theta)}{\partial \theta_i}=0
$$
> 准确来说，上式求出的是平稳值的位置，一般情况下从中挑选使得L最大的值即可。

为了方便计算，一般情形下，可以选取参数的函数进行处理，即：
$$
\frac{\partial \ln L(h)}{\partial h}=0, \, h = h(\theta)
$$

> 这里体现了使用最大似然估计的方法的优点，注意到<mark>这一估计量的结果与函数形式的选择无关</mark>，只要$h$关于$\theta$是“好的”（单调连续）。
>
> 最大似然法的好处：利用了所有数据的信息，与区间的划分无关，对测量值不做任何的修正。
>
> >关于“与分区间是否有关”这一点，可以通过与最小二乘法的对比来理解。对于最小二乘法在分区间的过程中，距离较近的点的值被合并，每个点的作用被削弱。但是最大似然法着重于“计数”的性质，合并区间并不会造成实质性的影响。

> 最大似然法的缺点：
>
>一般情况下最大似然法给出的结果不会非常糟糕（渐进（$n\to \infty$）有效），但是在个别极端情况下会带来问题（如$U[X]$，估计随机变量的范围）。同时，在参数较多的情形，这一方案的计算难度较大。

### 例子：指数分布与高斯分布的估计量确定
#### 指数分布
分布形式：
$$
f(t;\tau)= \frac{1}{\tau}\mathrm e ^{-\frac{t}{\tau}}
$$

给定实验样本之后：
$$
\ln L(\tau)=-\sum_{i=1}^n \frac{t_i}{\tau}-n\ln\tau
$$

> 对于实验样本，要求为<mark>简单数据样本</mark>，意味着各次测量独立同分布（iid，independent identical distribution）。

由最大似然法：

$$
\hat \tau = \frac{1}{n}\sum_{i=1}^nt_i
$$


##### 估计量的偏倚
由于形式是平均值的分布，所以应该是无偏估计量。这点证明起来也非常简单，直接计算平均值即可。

###### 另一种指数分布形式的表达
$$
f(t; \lambda)=\lambda\mathrm e^{-\lambda t}
$$

显然估计量为：

$$
\hat \lambda=\frac{1}{\frac{1}{n}\sum_{i=1}^nt_i}
$$

但是这一估计量仅仅是渐近无偏的。

> 使用特征函数的方法可以硬算得到概率密度分布函数，可以验证。

#### 高斯分布

$$
\ln L(\mu, \sigma^2)=\sum_{i=1}^n(\ln \frac{1}{\sqrt {2\pi }}+\frac{1}{2}\ln{\frac{1}{\sigma^2}}-\frac{(x_i-\mu)^2}{2\sigma^2} )
$$

得到：

$$
\hat \mu=\frac{1}{n}\sum_{i=1}^nx_i
$$

$$
\hat \sigma^2=\frac{1}{n}\sum_{i=1}^n(x_i-\hat \mu)^2
$$

> 显然，均值的估计量是无偏估计量，方差的估计量是渐近无偏估计量。

### 估计量的方差

#### 使用MC方法确定估计量的方差

具体而言，将估计值作为真值，进行多次MC模拟，计算数据样本的方差$s^2$近似作为估计量的方差的值。

#### 使用解析方法确定估计量的方差

对于常见的分布形式，直接积分计算估计量的方差是可行的，但是一般情形下的计算是困难的。

#### 使用RCF边界确定估计量的方差

信息不等式：

$$
V[\theta]\geq \frac{(1+\frac{\partial b}{\partial \theta})^2}{E[-\frac{\partial^2 \ln L}{\partial \theta^2}]}
$$

一般情况下，偏倚很小，可以忽略。如果最大似然估计量(ML估计量)为有效估计量，则
$$
V[\hat \theta]\approx\frac{1}{-E[-\frac{\partial^2 \ln L}{\partial \theta^2}]}
$$
对于高维情形：

![20250510101604](https://raw.githubusercontent.com/stur007/img/main/img/20250510101604.png)

> 这一近似也是合理的，毕竟是“最大”的概率密度.

#### 使用图解法确定估计量的方差
用ML估计量处的值估计不确定度，可以看出：
$$
\begin{aligned}
\ln L(\theta)&=\ln L(\hat \theta)+\frac{\partial \ln L(\theta)}{\partial \theta}|_{\theta = \hat\theta}(\theta - \hat \theta)+\frac{1}{2}\frac{\partial^2 \ln L}{\partial \theta^2}|_{\theta = \hat \theta}(\theta-\hat \theta)^2\\
&=\ln L(\hat \theta)-0.5\frac{(\theta-
\hat\theta)^2}{V[\theta]}
\end{aligned}
$$
可以看出，在图中$\ln L$下降为0.5时的半宽度为方差的估计值。

### 扩展的最大似然估计
解决的是样本容量不固定的情形。在实验过程中，样本空间的大小n也是随机变量（一定时间内衰变事例发生的次数）。容易看出，事例发生的次数（样本空间的大小）满足均值为$\nu$的泊松分布。

$$
L(\nu;\theta) = \frac{\nu^n}{n!}\mathrm e^{-\nu}\prod _{i=1}^n f(x_i;\theta)
$$

#### 均值与未知参数相关的情形

$$
\begin{aligned}
\ln L(\nu; \theta)&=n\ln\nu(\theta)-\nu(\theta)-\ln(n!)+\sum_{i=1}^n\ln f(x_i;\theta)\\
&=c-\nu(\theta)+\sum_{i=1}^n\ln(\nu(\theta)\cdot f(x_i, \theta))
\end{aligned}
$$

扩展的最大似然函数利用了样本空间大小的信息，理论上可以减小不确定度。

> 例子：
>
> $\vec x = (x_1, x_2, ..., x_n), X_i \sim N(\mu, \sigma^2)$，则均值$\sim N(\mu, \sigma^2/n)$。


#### 均值与未知参数无关的情形
此时：
$$
\ln L(\nu,\theta)=c(\nu)+\sum_{i=1}^n\ln f(x_i, \theta)
$$

对于$\theta$参量处理时，形式与样本空间的大小固定时相同，但是这样的形式有利于处理$f(x)$已知有不同成分叠加的情形。注意到各个$\theta_i$并不是独立的，之间有约束关系$\sum_i^m \theta_i=1$。但是加入事例总数作为变量以后这个问题就可以得到比较方便的解决。

> 例如，对于信号与本底的组合，对于概率密度函数$f(x)=\frac{\mu_s}{\mu_s+\mu_b}f_s(x)+\frac{\mu_b}{\mu_s+\mu_b}f_b(x)$，使用扩展的极大似然估计，可以直接估计出信号与本底的事例数$\mu_s$，$\mu_b$。

> 对于非物理结果，（本底信号较大时信号的估计值时是负数），也应该按照统计涨落来正常记录。（在这一假设下）


### 分区间数据的极大似然估计

在数据分区间的情形下，可以对分区间数据进行拟合。根据假设，得到预期的区间内的事例数：$\vec \nu = (\nu_1, ..., \nu_n)$，似然函数为多项分布，对数似然函数的结果为：

$$
\ln L = \sum_{i=1}^nn_i\nu_i(\theta)
$$
> 相比于点估计(unbinned ML)，误差稍大，但是影响不大。

### ML拟合优度检验

解析计算拟合的优度一般是复杂的。对于拟合优度检验，一般直接利用ML方法对$\ln L$进行模拟，p值的确定就是$P(\ln L\leq\ln L_{actual})$。

### 不等精度似然结果的合并
假设对$\mu$进行<mark>独立</mark>测量$n$次，测量结果为$x_i\pm\sigma_i$,且$x_i\sim N(\mu, \sigma_i^2)$。使用最大似然法对$\mu$进行估计，得到：

$$
\ln L(x_i, \mu)=c-\frac{1}{2}\sum_{i=1}^n\frac{(x_i-\mu)^2}{\sigma_i^2}
$$

$$
\frac{\partial \ln L}{\partial \mu}=0\Rightarrow\sum_{i=1}^n\frac{x_i-\hat \mu}{\sigma_i^2}=0
$$

$$
\hat \mu=\frac{\sum_{i=1}^nw_ix_i}{\sum_{i=1}^nw_i},\,w_i = \frac{1}{\sigma_i^2}
$$

由各个测量量相互独立，可以方便的计算出
$$
\sigma_{\hat \mu}=\frac{1}{\sum_{i=1}^nw_i}
$$

> 对所有测量结果的合并，不确定度一定比任何一次测量的结果更低。

>对于权重因子也可以按照这种方式进行定义，本质上其实是完全相同的。
>
>![20250515154116](https://raw.githubusercontent.com/stur007/img/main/img/20250515154116.png)
## chap 07 最小二乘法
### 最小二乘法与最大似然法之间的关系
对于$n$个相互独立的高斯变量$Y_i$，对数似然函数可以直接等价于：

$$
\chi^2= \sum_{i=1}^n\frac{(y_i-\lambda(x_i;\theta))^2}{\sigma_i^2}取最小值。
$$

$\lambda_i$为待确定的真值，$\lambda_i =\lambda(x_i; \theta)$，$x_i$认为是精确测量的结果。

对于并不相互独立的高斯变量，但是可以用高维高斯密度函数分布描述的结果，可以写为：
$$
\chi^2=\sum_{i,j = 1}^n(y_i-\lambda(x_i;\theta))V^{-1}_{ij}(y_j-\lambda(x_j;\theta))取最小值。
$$

对于不是高斯分布的情形（由中心极限定理，一般情形下$y_i$偏离高斯变量不会太多），也可以使用$\chi^2$最小值来确定参数的取值。
### 线性情形下的最小二乘估计
对于$\lambda$对各个**参数**$\theta_j,\,j=1, .., m$线性的情形：

$$
\lambda_i = \sum_{j=1}^ma_j(x_i)\theta_j,\,a_j(x)是任意线性独立函数。
$$

此时得到的估计量是有效估计量。（无偏且方差最小）

设$\bold A:A_{ij}=a_j(x_i)$，则$\vec \lambda = \bold A\vec \theta$。

$$
\begin{aligned}
\chi^2&=(\vec y-\bold A\vec \theta)^T\bold V^{-1}(\vec y-\bold A\vec \theta)\\
\frac{\partial \chi^2}{\partial \vec \theta}&=-\bold A^T\bold V^{-1}(\vec y-\bold A\vec \theta)-(\vec y-\bold A\vec\theta)^T\bold V^{-1}\bold A=0\\
\Rightarrow \hat {\vec \theta} &=(\bold A^T\bold V^{-1}\bold A)^{-1}\bold A^T\bold V^{-1}\vec y=:\bold B \vec y
\end{aligned}
$$

#### 估计量的（协）方差

$$
\bold U = \bold {BVB^T}=(\bold {AV^{-1}A^T})^{-1}=(\frac{1}{2}\nabla\nabla\chi^2)^{-1}
$$

换言之，
$$
\chi^2(\hat{\vec\theta}\pm\sigma_{\hat{\vec\theta}})=\chi_{min}^2+1
$$

> 在高斯分布的形式下，由于$\chi^2=-2\ln L+c$，可以对应于最大似然法的图解法条件。

> 这一范围确定下来的$\vec \theta$区间称为置信区间。

### 多项式的最小二乘拟合
作为线性估计量的一个实例，考虑多项式的假设为：

$$
\lambda(x_i;\vec\theta)=\sum_{i=1}^m\theta_ix_i^{i-1}
$$

当多项式自由变量$m+1=n$时，$\chi^2=0$恒成立，估计无意义。（过拟合）

> 非线性最小二乘法拟合估计：一般情形$\lambda(x;\vec\theta)$下只能进行数值求解。

> 有约束情形下的最小二乘拟合
> 
>为了简单起见，仅考虑无参数的最小二乘拟合。使用Lagrange乘子法计算
>
> $$
> \chi^2(\vec x, \vec \alpha)=(\vec x ^\prime -\vec x )^T\bold V^{-1}(\vec x^\prime -\vec x)+2\vec \psi^T\alpha
> $$
>
> 的最小值即可。

### 检验最小二乘法的拟合优度
$\chi_{min}^2$的大小决定了数据与假设之间的符合程度，进而可以直接用来检验拟合优度。

如果$n$个变量相互独立的高斯分布变量，由定义$\chi^2$显然满足自由度为<mark>$(n-m)$</mark>的卡方分布。

如果$\chi_{\min}^2$较小，只能说明实验数据与当前假设相对符合。

> ==“较小”的标准:$E[\chi^2(n_d)]=n_d$==，认为是可以接受的结果。

> 拟合优度与统计不确定度之间的关系：统计不确定度对应于$\chi^2$在最小值附近的变化快慢，拟合优度对应$\chi^2_{\min}$的大小。换言之，拟合优度与测量的不确定度直接并==没有直接的关系==。
>
> 例如，如果对某次测量的结果进行上下“平移”的同时保持误差不变，对于参数的估计值的误差保持不变，但是$\chi^2_{\min}$的值可以变大或变小。

### 最小二乘法处理分区数据
对直方图进行拟合。假设$X$的概率密度分布函数$f(x;\theta)$，$Y_i$为第$i$个区间的频数，则$\lambda_i=n\int_{x_\min}^{x_\max} f(x;\theta)\mathrm d x=np_i(\vec \theta)$。假设区间的宽度足够小，则$Y_i$可以看作是泊松变量。
$$
\chi^2(\theta) = \sum_{i=1}^n\frac{(y_i-\lambda_i)^2}{\sigma_i^2}
$$

> 换言之，假定$y_i$服从均值为$\lambda_i(\theta)$，方差为$\sigma_i^2(\theta)$的泊松分布，这种方法称为最小二乘估计(LS)。在实际操作中，常常使用$\hat \sigma_i^2=y_i$代替$\sigma_i^2$，这种方法称为修正最小二乘估计(MLS)。MLS估计方法的优点是，计算比较方便，但缺点在于，如果区间的数据量太小3.，$\sigma_i^2$的估计会相对不准。一般情形下，$y_i>5$就可以认为是相对不错的估计量。

对比扩展的最大似然法（将事例总数作为泊松变量进行估计），在最小二乘估计中使用类似的方案，得到：
$$
\lambda_i(\theta)=\nu p_i(\theta)
$$

$$
\chi^2(\theta; \nu)=\sum_{i=1}^n\frac{(y_i-\lambda_i)^2}{\sigma_i^2}\\
\frac{\partial\chi_{MLS}^2}{\partial\nu}=0\Rightarrow \sum_{i=1}^n\frac{(y_i-\lambda_i)(-p_i)}{y_i}=0,\,\hat \nu_{MLS} = \frac{\sum_{i=1}^np_i}{\sum_{i=1}^np_i^2/y_i}=\frac{1}{\sum_{i=1}^np_i^2/y_i}\\
\frac{\partial \chi_{LS}^2}{\partial \nu}=0\Rightarrow\sum_{i=1}^n\frac{y_i^2}{\nu p_i}+p_i=0,\,\hat \nu_{LS}^2=\sum_{i=1}^n\frac{y_i^2}{p_i}
$$

代入实际的估计量的值的结果，得到：
$$
\hat \nu_{MLS}=n-\chi^2(\theta; \hat\nu_{MLS})\\
\hat \nu_{LS}=n+\chi^2(\theta; \hat \nu_{LS})/2
$$
> 证明过程是容易的，将$\hat \nu$的表达式代入$\chi^2$中即可证明，可以注意到这个公式是对于任意的$\theta$均成立的。

不是好的估计量，有偏倚。

### 测量结果的合并

对于$n$次测量，测量结果为$\vec y  =(y_1, .., y_n)$，协方差矩阵为$\bold V^{n\times n}$。如果各个测量结果相互独立，用最小二乘法估计真值：
$$
\chi^2=\frac{(y_i-\lambda)^2}{V_{ii}},\,\frac{\partial \chi^2}{\partial \lambda}=0\Rightarrow \hat \lambda=\frac{\sum_{i=1}^n\frac{y_i}{V_{ii}}}{\sum_{i=1}^n\frac{1}{V_{ii}}}
$$
这与使用ML方法得到的结果相同。

而对于相关的测量结果：
$$
\chi^2= \vec{y^\prime}^T\bold V^{-1}\vec {y^\prime} =\sum_{i, j=1}^n(y_i-\lambda)V^{-1}_{ij}(y_j-\lambda),\,\frac{\partial \chi^2}{\partial \lambda}=0\\\Rightarrow\hat \lambda=\frac{\sum_{i=1}^n\omega_i y_i}{\sum_{i=1}^n\omega_i},\,\omega_i=\sum_{j=1}^nV^{-1}_{ij},V[\lambda]=\frac{\sum_{i=1}^n\omega_i V_{ij}\omega_j}{\sum_{i=1}^n\omega_i}
$$

> 例子：对两个（相关）的实验结果进行合并，等价于拟合平均值（斜率为0的拟合，即单参数拟合）。
>
> ![image-20250522165330330](https://raw.githubusercontent.com/stur007/img/main/img/202505221653138.png)
>

> 考虑$X_1= x_1\pm\sigma^{stat}_1\pm\sigma^{syst}_1, \, X_2= x_2\pm\sigma^{stat}_2\pm\sigma^{syst}_2$，则协方差矩阵：
> $$
> \bold V = 
> \left[ 
> \begin{aligned}
> \sigma_1^2  ~~~~~\sigma_1^{syst}\sigma_2^{syst} \\
> \sigma_1^{syst}\sigma_2^{syst}~~~~~~~\sigma_2^2 \\
> 
> \end{aligned}
> \right]
> $$
>
> 其中：
> $$
> \sigma_i^2 = {\sigma_i^{stat}}^2+{\sigma_i^{syst}}^2, i = 1, 2
> $$

## chap 08 矩方法

### 矩的定义

#### 原点矩（代数矩）

$$
\mu_k^\prime = \int f(x)x^k\mathrm d x
$$

#### 中心矩

$$
\mu_k = \int f(x)(x-E[X])^k\mathrm d x
$$

#### 中心矩与原点矩之间的关系

$$
\mu_k = \sum_{i=0}^k(-1)^{n-i}{\mu_n^\prime}^iE[X]^{k-i}
$$

一般情形下，可以使用给出的特征函数方便的计算出各阶原点矩。
$$
\varphi(t)=E[\mathrm e^{\mathrm i tX}]=\int f(x)\mathrm e^{\mathrm i tx}\mathrm d x
$$

$$
\frac{\mathrm d^k \varphi (t)}{\mathrm d t^k}=\mathrm i^k\int f(x)x^k\mathrm e ^{\mathrm i t x}\mathrm d x
$$

故
$$
\mu_k^\prime=\frac {\varphi^{(k)}(0)}{\mathrm i^k}
$$

### 矩方法的估计量

$$
E[a_i(x)] =\int f(x)a_i(x;\vec \theta)\mathrm d x=e_i(\vec\theta)
$$

使用$a_i(x)$的样本平均对$e_i(\theta)$做出估计，用数学语言表达：
$$
\hat e_i(\vec \theta)=\frac{1}{n}\sum_{j=1}^n a_i(x_j)
$$
得到m个方程，原则上可以确定m个参数的估计值。一般情形下，可以直接$a_i=x^i$，这也是矩方法的来源。

#### 估计量的（协）方差

对于估计量$\hat {\vec \theta}$，可以反解出:
$$
\hat \theta_i = \hat \theta_i(\hat e_i)
$$
使用误差的传递公式：
$$
\text{cov}(\theta_i, \theta_j)=\frac{\partial \theta_i}{\partial e_m}\frac{\partial \theta_j}{\partial e _n}\text{cov}(e_m, e_n)
$$
使用样本的协方差估计总体的协方差：
$$
\text{cov}(e_m,e_n)=\text{cov}(\overline a_m, \overline a_n)=\frac{1}{n-1}\sum_{i=1}^n(a_m(x_i)-\overline a_m)(a_n(x_i)-\overline a_n)
$$

> 注意，估计量的（协）方差并不等价于方差的估计量。使用矩方法给出的方差的估计量的形式是除以1/n的，不是无偏的估计量。估计量的方差不是用矩方法给出的，是使用误差传递公式给出的，那当然要使用最准确的公式进行估计了。
>
> 对于下面的例子，情况又有所不同。这是一个假设检验问题，不需要进行估计，所以直接使用样本方差来衡量是否符合就行。

> ![image-20250529154104681](https://raw.githubusercontent.com/stur007/img/main/img/202505291611204.png)
>
> ![image-20250529154154982](https://raw.githubusercontent.com/stur007/img/main/img/202505291611711.png)
>
> ![image-20250529154213688](https://raw.githubusercontent.com/stur007/img/main/img/202505291612507.png)

### 矩方法与最小二乘法、最大似然法的对比

![image-20250529160025658](https://raw.githubusercontent.com/stur007/img/main/img/202505291612897.png)

## chap 09 统计误差、置信区间和极限

### 统计不确定度中的标准差问题 

例子：$\hat \theta=\hat \theta(\vec x )$是随机变量，那么
$$
\hat \theta \pm \hat \sigma_{\hat \theta}
$$
意味着$\theta$的估计值是$\hat \theta$，$\sigma_\theta$的估计值是$\hat\sigma_\theta$。从方差的估计情况可以判断出$\theta$的分布情况。

#### 点估计与区间估计

- 点估计：在某一方法下最好的估计值。
- 区间估计：真值落在某一区间的范围大。

### 经典置信区间问题  

#### 置信区间

对于给定的小量$\alpha$，$\theta$为待定的估计量，$\Theta$为所有的估计量取值构成的集合。对于$\theta \in \Theta$满足
$$
P(T_1<\theta<T_2)\geq 1-\alpha
$$
那么随机区间$[T_1, T_2]$称为置信区间。等号成立时称为**同等置信区间**。

> $T_1(置信下界), T_2(置信上界)$是数据样本的随机变量。

#### 置信区间的含义

<mark>频率论的观点：对于足够多次实验，每次用相同的方法构造置信区间，真值落在置信区间内的概率为$1-\alpha$。</mark>

贝叶斯的观点：有$1-\alpha$的信心确定真值落在置信区间中。

> 置信区间的精度与可靠度：
>
> 区间长度越小，说明区间的精度越高；置信区间的$\alpha$越小，说明置信区间的可靠度越高。

#### 区间估计的基本步骤

- 确定枢轴量

> **枢轴量：仅仅是待估参数与样本的函数。同时，枢轴量的分布*不能*依赖于待估计的参数。**

- 构建不等式
- 解出置信区间

### 例子：正态分布均值的区间估计

对于独立同分布的正态分布随机变量$X\sim N(\mu, 1^2)$。样本空间为$X_i, i=1,2,3,4,5$。容易注意到：
$$
\frac{\overline X-\mu}{\sqrt{1/5}}\sim N(0, 1)
$$
根据给定的$\alpha$，可以给出置信区间为$[\overline X-\delta(\alpha), \overline X+\delta(\alpha)]$。

>另一种方法：对于高斯随机随机变量
>$$
>\sum_{i=1}^n\frac{(X_i-\mu)^2}{\sigma_i^2}
>$$
>服从自由度为$n$的卡方分布。*可以验证，采用不同的枢轴量和方法，给出的置信区间是可以不同的。*

### 置信区间的构造

- 一般情况下，置信区间的端点取值<mark>并不唯一</mark>。如果有可能，应当选取最短的置信区间。
- 对于偏态分布的情形，选取最短的置信区间往往是不容易做到的，常常选取两侧对称的概率（均为$\frac{\gamma}{2}$），称为**等尾置信区间**。

#### 对于一般情况的讨论

对于构造的估计量，可以给出估计量随真值的变化函数$g(\hat \theta;\theta)$。对于双侧边界给出的$\alpha,\beta$：
$$
\alpha = \int_{-\infty}^{u_\alpha(\theta)} g(\hat \theta;\theta)\mathrm d \theta
$$

$$
\beta =\int_{v_\beta(\theta)}^\infty g(\hat \theta;\theta)\mathrm d \theta
$$

由两个积分就可以方便的求出上下界$u_\alpha(\theta),\,v_\beta(\theta)$。在$u_\alpha(\theta)$到$v_\beta(\theta)$之间的区域称为置信带。

对于某次实验的估计量$\hat\theta$，利用方程：
$$
u_\alpha(a)=\hat\theta
$$

$$
v_\beta(b)=\hat \theta
$$

确定置信区间为$[b, a]$。

### 高斯分布估计量的置信区间

如果$g(\hat \theta;\theta)$服从高斯分布，而且认为$\sigma_{\hat \theta}$是已知的，则置信区间$[b, a]$为：
$$
b = \hat \theta-\sigma_{\hat\theta}\Phi^{-1}(1-\alpha)\\
a = \hat \theta+\sigma_{\hat\theta}\Phi^{-1}(1-\beta)
$$

### 泊松分布估计量的置信区间
考虑估计量$n$，服从均值为$\nu$的泊松分布：
$$
P(n; \nu)= \frac{\nu^n}{n!}\mathrm e ^{-\nu}
$$
估计：
$$
P(x \geq n; a) = \sum_{i=n}^{\infty}\frac{a^i}{i!}\mathrm e ^{-a}\geq\alpha 
$$

$$
P(x \leq n; b)= \sum_{i=0}^n\frac{b^i}{i!}\mathrm e ^{-b} \geq \beta
$$

a与b是满足上述不等式的最小（最大）整数。
注意到卡方分布与泊松分布之间的关系，可以方便的定出结果。

具体结果：
![20250605163338](https://raw.githubusercontent.com/stur007/img/main/img/20250605163338.png)

> 例子：泊松分布均值置信水平的上限。测量结果为$n = 0$。此时，$P(x\leq n; b)=\mathrm e ^b= \beta$，即$b = -\ln \beta$。常见的结果是，置信水平$1-\beta = 95\%$的结果为3。
>
> 公式的证明：
>
> ![image-20250616111531682](https://raw.githubusercontent.com/stur007/img/main/img/202506161115362.png)


### 置信区间与假设的接受域之间的关系
参数的置信区间可以通过定义参数的假设检验得到。

> 换言之，对于假设$\mu = \mu_0$，实验过程中观测到的结果为$\hat \mu$。假设不被拒绝，应当等价于假设$\mu = \mu_0落在置信区间内。

假设接受域：如果假设$\theta$正确，则观测到的结果$n$落在接受域之外的概率为$\alpha$。

置信区间：观测结果为$n$，则真值$\theta$落在置信区间范围之外的概率为$\alpha$。

例子：对于高斯分布，考虑$n$个独立同分布(independent identical distribution, iid)的变量$X_1, ..., X_n$，容易发现，上述两个定义是等价的，而且，两个区间的长度也是一样的。

### 物理发现或上限的检验统计量 

对于在测量之前就知道相关信息的统计量， 我们可以使用Bayes验前概率的方案进行处理，得到更好的实验结果。下面的例子是含有信号和本底的信号估计上限。不妨认为本底的分布情况已知，为均值为b的泊松分布，注意到s的分布一定是大于0的，所以有：
$$
P(n|s)=\frac{(s+b)^n}{n!}\mathrm e^{-(s+b)}
$$
s的验前概率密度（含有待定的归一化常数）可以取为：
$$
\pi (s) = c, s>0
$$
通过Bayes概率公式可以得到：
$$
p(s|n)= \frac{p(n|s)\pi (s)}{\int p(n|s)\pi(s)}
$$
再利用置信区间的定义，可以求出信号的上限。

> 反思：之前在考虑这一问题时，自动默认验前概率是均匀的，这一点是否合理呢？
>
> （用形式规则确定验前概率，时间有限没讲到）

### 常见的分布补充

#### $F$分布

$X\sim \chi^2(n), Y\sim \chi^2(m)$,则$
\frac{X/n}{Y/m}$服从F分布。

#### $t$分布

假设X服从标准正态分布$N(0,1)$，Y服从$\chi^2(n)$分布，那么$Z=\frac{X}{\sqrt{Y/n}}$的分布称为自由度为$n$的$t$分布,记为$t(n)$。

##### $t$分布的应用

对于相互独立的服从高斯分布的n个量而言，均值与方差分别满足：
$$
\frac{\overline X-\mu }{\sigma/\sqrt{n} }\sim N(0, 1)
$$

$$
(n-1)S^2/\sigma^2 \sim \chi^2(n-1)
$$

而且两者相互独立。

所以
$$
\frac{\overline X-\mu }{S/{\sqrt n}}\sim t(n-1)
$$

## 附录：概率论沉思录

### chap 01 合情推理

1. 强三段论（演绎推理）：A真 -> B真，A真，则B真

   弱三段论（合情推理）：A真 -> B真，B真，则A更合情

2. 布尔代数：

   积：AB（A与B同时为真，则为真）

   和：A+B（A或B为真，则为真）

   否：$\overline{A}$ （A为假）

   > - 写成加法与乘法的形式，为了直接运用加法与乘法的运算律。
   >
   > - 运算的实例：
   >   - $AA=A$，$A \overline A =False(0)$，$0 + A = A$，$\overline{\overline{A}}=A$，$A+A=A$
   >   - $A+\overline{A}=True(1)$，$True(1)A=A$
   >   - $\overline{A}+A\overline{B}=\overline{AB}$

3. 蕴含关系

   $A\Rightarrow B$ （A为真，则B为真；B为假，则A为假）

   用布尔代数表达：$A = AB$ ，可以验证满足上面的蕴含关系的定义。

   > $A\Rightarrow B$的布尔值与$\overline{A}+B$的布尔值相同。

4. 逻辑函数

   对$n$个命题，最多有$2^{n+1}$个不同的概率函数。（n个命题有$2^n$种结果，函数可能有两种取值）

   > $2^{n+1}$种函数，可以简化为规范析取范式（形如$A_1...A_n$，$\overline{A_1}...A_n$的合取（交/乘积）式的析取（或/和）式）

5. 完备集：合取(AND)、析取(OR)与否定(NOT)

   > 实际上这三种逻辑关系中，两者可以表出第三者，如$A+B=\overline{\overline{A}~\overline{B}}$
   >
   > ，$AB=\overline{\overline{A}+\overline{B}}$。进而可以定义与非（$\uparrow$）运算：$A\uparrow B=\overline{A}+\overline{B}$（合取式的否定），或非（$\downarrow$）运算：$A\downarrow B= \overline{A}~\overline{B}$（析取式的否定）

6. 合情条件

   - 合情程度用实数表示$(A|B)$
   - 定性地与常识相符
   - 一致性推理（结果与路径无关）

### chap 02 定量规则

1. 乘法规则

   $(AB|C)=F[(B|C), (A|BC)]$

   $(AB|C)=F[(A|C), (B|AC)]$

进一步确定$F(x, y)$的形式：

- 由合情条件I：$F_x\geq 0, F_y\geq 0$

- 由迭代关系：$F[x, F(y, z)]=F[F(x, y),z]$

  记$u=F(x,y),v=F(y,z)$，对上面的表达式分别对$x,~y$求偏导：
  $$
  F_1(u,z)F_1(x,y)=F_1(x,v)
  $$

  $$
  F_1(u,z)F_2(x,y)=F_2(x,v)F_1(y,z)
  $$

  记$G(x,y)=\frac{F_2(x,y)}{F_1(x,y)}$
  $$
  G(x,y)=G(x,v)F_1(y,z)......(\#)
  $$
  或
  $$
  G(x,y)G(y,z)=G(x,v)F_2(y,z)......(\#\#)
  $$
  对（#）式，对求z偏导：
  $$
  0=G_2(x,v)F_2(y,z)F_1(y,z)+G(x,v)F_{12}(y,z)
  $$
  对（##）式，对y求偏导：
  $$
  [G(x,y)G(y,z)]_y=G_2(x,v)F_1(y,z)F_2(y,z)+G(x,v)F_{21}(y,z)
  $$
  连续性给出：
  $$
  F_{12}(y,z)=F_{21}(y,z)
  $$
  代入得到：
  $$
  [G(x,y)G(y,z)]_y=0
  $$
  进而可以确定函数$G(x,y)$的形式：
  $$
  G(x,y)=r\frac{H(x)}{H(y)}
  $$
  将分离变量的形式代入（#）（##）式，略去推导，积分得到：
  $$
  \phi(v) = c\phi(y)\phi^r(z)
  $$
  其中：
  $$
  \phi(x)=\int\frac{\mathrm{d}x}{H(x)}
  $$
  将这一结果代入迭代关系的表达式，化简得到：
  $$
  \phi(F(x,y))=c\cdot\phi(x)\phi(y)
  $$
  令$w(x)=c\cdot \phi(x)$，得到：
  $$
  w(F(x,y))=w(x)w(y)
  $$
  回到原来的问题，得到结论：
  $$
  w(AB|C)=w(A|BC)w(B|C)
  $$

2. 加法规则

   略去推导，用类似的方式可以推出：
   $$
   P(A|B)+P(\overline{A}|B)=1
   $$

3. 推理规则与演绎推理、合情推理之间的关系（合情推理的合情程度用概率函数表达）